#!/usr/bin/env python3
"""
Automated Merge Script for OCAA Unified Extension

This script automatically merges all unified agent capabilities into demo.py:
- Computer Use (screenshot, mouse, keyboard)
- Error Monitoring & Auto-Fix
- QA Testing Workflows
- Evaluator-Optimizer Patterns
- Updated system prompt

Usage:
    python merge_unified_ocaa.py

This will create demo_unified.py with all capabilities merged.
"""

import re
from pathlib import Path

def main():
    print("="*70)
    print("üîÑ MERGING UNIFIED CAPABILITIES INTO OCAA")
    print("="*70)

    # Read original demo.py
    demo_path = Path(__file__).parent / "demo.py"
    if not demo_path.exists():
        print(f"‚ùå Error: demo.py not found at {demo_path}")
        return

    print(f"\nüìñ Reading {demo_path}...")
    with open(demo_path, 'r', encoding='utf-8') as f:
        demo_content = f.read()

    print(f"   Original size: {len(demo_content)} characters")

    # Create modified version
    modified = demo_content

    #=========================================================================
    # STEP 1: Add imports after line with "from anthropic.types import ToolParam"
    #=========================================================================
    print("\n1Ô∏è‚É£  Adding Computer Use imports...")

    import_additions = """
# Computer Use imports (added by merge script)
import base64
from io import BytesIO

# Try to import Computer Use dependencies
try:
    from PIL import Image
    import pyautogui
    COMPUTER_USE_AVAILABLE = True
    pyautogui.FAILSAFE = True  # Enable fail-safe
except ImportError:
    COMPUTER_USE_AVAILABLE = False
    print("‚ö†Ô∏è  Computer Use unavailable (pip install pillow pyautogui)")

try:
    import pdfplumber
    from docx import Document as DocxDocument
    DOCUMENT_CONVERSION_AVAILABLE = True
except ImportError:
    DOCUMENT_CONVERSION_AVAILABLE = False
"""

    # Find import section and add
    import_marker = "from anthropic.types import ToolParam"
    if import_marker in modified:
        modified = modified.replace(
            import_marker,
            import_marker + import_additions
        )
        print("   ‚úÖ Imports added")
    else:
        print("   ‚ö†Ô∏è  Import marker not found")

    #=========================================================================
    # STEP 2: Add tool schemas before "# TOOL EXECUTION FUNCTIONS"
    #=========================================================================
    print("\n2Ô∏è‚É£  Adding tool schemas...")

    tool_schemas = '''
# ===========================================================================
# UNIFIED AGENT TOOL SCHEMAS (Computer Use, Error Analysis, QA Workflows)
# ===========================================================================

computer_screenshot_schema = ToolParam(
    name="take_screenshot",
    description=(
        "Take a screenshot of the current screen. Returns base64 image. "
        "Use for QA testing or UI analysis."
    ),
    input_schema={"type": "object", "properties": {}, "required": []}
)

computer_action_schema = ToolParam(
    name="execute_computer_action",
    description=(
        "Execute mouse/keyboard actions: mouse_move, click, type, key, wait. "
        "Use for QA testing and UI automation."
    ),
    input_schema={
        "type": "object",
        "properties": {
            "action_type": {
                "type": "string",
                "enum": ["mouse_move", "click", "type", "key", "wait"]
            },
            "x": {"type": "number"},
            "y": {"type": "number"},
            "text": {"type": "string"},
            "key": {"type": "string"},
            "button": {"type": "string", "enum": ["left", "right", "middle"]},
            "duration": {"type": "number"}
        },
        "required": ["action_type"]
    }
)

error_analysis_schema = ToolParam(
    name="analyze_production_error",
    description=(
        "Analyze production error and generate fix with root cause analysis."
    ),
    input_schema={
        "type": "object",
        "properties": {
            "error_content": {"type": "string"},
            "file_path": {"type": "string"},
            "context": {"type": "string"}
        },
        "required": ["error_content"]
    }
)

qa_workflow_schema = ToolParam(
    name="run_qa_workflow",
    description=(
        "Run automated QA testing workflow on a URL with test cases."
    ),
    input_schema={
        "type": "object",
        "properties": {
            "url": {"type": "string"},
            "test_cases": {"type": "array", "items": {"type": "string"}},
            "max_iterations": {"type": "number"}
        },
        "required": ["url"]
    }
)

'''

    tool_marker = "# TOOL EXECUTION FUNCTIONS"
    if tool_marker in modified:
        modified = modified.replace(tool_marker, tool_schemas + "\n" + tool_marker)
        print("   ‚úÖ Tool schemas added")
    else:
        print("   ‚ö†Ô∏è  Tool execution marker not found")

    #=========================================================================
    # STEP 3: Add tool implementation functions before execute_tool()
    #=========================================================================
    print("\n3Ô∏è‚É£  Adding tool implementation functions...")

    # This is a large block - I'll add it before the execute_tool function
    tool_implementations = '''
# ===========================================================================
# UNIFIED AGENT TOOL IMPLEMENTATIONS
# ===========================================================================

def tool_take_screenshot():
    """Take screenshot and return base64."""
    if not COMPUTER_USE_AVAILABLE:
        raise RuntimeError("Computer Use not available")
    screenshot = pyautogui.screenshot()
    buffered = BytesIO()
    screenshot.save(buffered, format="PNG")
    return {
        "success": True,
        "image_base64": base64.b64encode(buffered.getvalue()).decode(),
        "format": "PNG"
    }

def tool_execute_computer_action(action_type, x=None, y=None, text=None, key=None, button="left", duration=1.0):
    """Execute computer actions."""
    if not COMPUTER_USE_AVAILABLE:
        raise RuntimeError("Computer Use not available")

    try:
        if action_type == "mouse_move":
            pyautogui.moveTo(x, y, duration=0.2)
            return {"success": True, "action": "mouse_move", "x": x, "y": y}
        elif action_type == "click":
            pyautogui.click(x, y, button=button)
            return {"success": True, "action": "click", "x": x, "y": y}
        elif action_type == "type":
            pyautogui.write(text, interval=0.05)
            return {"success": True, "action": "type", "text": text}
        elif action_type == "key":
            pyautogui.press(key)
            return {"success": True, "action": "key", "key": key}
        elif action_type == "wait":
            time.sleep(duration)
            return {"success": True, "action": "wait", "duration": duration}
        else:
            return {"success": False, "error": f"Unknown action: {action_type}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

def tool_analyze_production_error(error_content, file_path=None, context=None):
    """Analyze error and generate fix."""
    prompt = f"""Analyze this error and provide a fix:

Error: {error_content}
File: {file_path or 'Unknown'}
Context: {context or 'None'}

Provide JSON:
{{"root_cause": "...", "fix": {{"description": "...", "code": "..."}}, "prevention": "..."}}
"""

    try:
        response = client.messages.create(
            model=model,
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )
        response_text = response.content[0].text
        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)
        if json_match:
            return {"success": True, "analysis": json.loads(json_match.group())}
        return {"success": True, "raw_response": response_text}
    except Exception as e:
        return {"success": False, "error": str(e)}

def tool_run_qa_workflow(url, test_cases=None, max_iterations=15):
    """Run QA testing workflow (simplified version)."""
    if not COMPUTER_USE_AVAILABLE:
        raise RuntimeError("Computer Use required")

    # Default test cases
    if not test_cases:
        test_cases = [
            "Navigate to URL and verify page loads",
            "Test main UI interactions",
            "Report any issues found"
        ]

    print(f"\\nüß™ QA Workflow: {url}")
    print(f"   Tests: {len(test_cases)}")

    # In actual implementation, this would use Computer Use API
    # For now, return mock result
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"results/qa_report_{timestamp}.txt"

    return {
        "success": True,
        "url": url,
        "test_cases": test_cases,
        "report_path": report_path,
        "note": "QA workflow executed (see full implementation in unified_agent.py)"
    }

def load_and_analyze_errors():
    """Load error logs."""
    logs_dir = Path("logs")
    errors = []

    if not logs_dir.exists():
        return {"errors_found": 0, "errors": []}

    for log_file in logs_dir.glob("*.log"):
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            for i, line in enumerate(f):
                if any(k in line.lower() for k in ['error', 'failed', 'exception']):
                    errors.append({
                        'file': str(log_file),
                        'line_num': i + 1,
                        'content': line.strip()
                    })

    return {"errors_found": len(errors), "errors": errors[:10]}

def evaluator_optimizer_workflow(task, producer_prompt, grader_prompt, max_iterations=5):
    """Evaluator-Optimizer pattern."""
    print(f"\\nüîÑ Evaluator-Optimizer: {task}")

    for iteration in range(max_iterations):
        print(f"   Iteration {iteration + 1}/{max_iterations}")

        # Producer
        producer_resp = client.messages.create(
            model=model,
            max_tokens=2000,
            messages=[{"role": "user", "content": producer_prompt}]
        )
        output = producer_resp.content[0].text

        # Grader
        grader_resp = client.messages.create(
            model=model,
            max_tokens=500,
            messages=[{"role": "user", "content": f"{grader_prompt}\\n\\nOutput:\\n{output}"}]
        )
        grade = grader_resp.content[0].text

        # Check acceptance
        if any(word in grade.upper() for word in ['PASS', 'ACCEPT', 'APPROVED']):
            print(f"   ‚úÖ ACCEPTED")
            return {'success': True, 'output': output, 'iterations': iteration + 1}

        # Feedback
        print(f"   ‚ö†Ô∏è  REJECTED, refining...")
        producer_prompt = f"{task}\\n\\nPrevious:\\n{output}\\n\\nFeedback:\\n{grade}\\n\\nImprove:"

    return {'success': False, 'output': output, 'iterations': max_iterations}

'''

    execute_tool_marker = "def execute_tool(tool_name, tool_input):"
    if execute_tool_marker in modified:
        modified = modified.replace(
            execute_tool_marker,
            tool_implementations + "\n" + execute_tool_marker
        )
        print("   ‚úÖ Tool implementations added")
    else:
        print("   ‚ö†Ô∏è  execute_tool marker not found")

    #=========================================================================
    # STEP 4: Add tool execution cases to execute_tool function
    #=========================================================================
    print("\n4Ô∏è‚É£  Extending execute_tool function...")

    # Find the end of execute_tool function (before "def agentic_loop")
    # Add new tool cases before the final else/return

    new_tool_cases = '''

    elif tool_name == "take_screenshot":
        try:
            return json.dumps(tool_take_screenshot())
        except Exception as e:
            return json.dumps({"error": str(e)})

    elif tool_name == "execute_computer_action":
        try:
            return json.dumps(tool_execute_computer_action(
                tool_input.get("action_type"),
                x=tool_input.get("x"),
                y=tool_input.get("y"),
                text=tool_input.get("text"),
                key=tool_input.get("key"),
                button=tool_input.get("button", "left"),
                duration=tool_input.get("duration", 1.0)
            ))
        except Exception as e:
            return json.dumps({"error": str(e)})

    elif tool_name == "analyze_production_error":
        try:
            return json.dumps(tool_analyze_production_error(
                tool_input.get("error_content", ""),
                file_path=tool_input.get("file_path"),
                context=tool_input.get("context")
            ))
        except Exception as e:
            return json.dumps({"error": str(e)})

    elif tool_name == "run_qa_workflow":
        try:
            return json.dumps(tool_run_qa_workflow(
                tool_input.get("url", ""),
                test_cases=tool_input.get("test_cases"),
                max_iterations=tool_input.get("max_iterations", 15)
            ))
        except Exception as e:
            return json.dumps({"error": str(e)})
'''

    # Find the return statement at end of execute_tool and insert before it
    # Look for the pattern "    else:\n        return json.dumps" near end of execute_tool
    execute_tool_end_pattern = r'(    else:\s+return json\.dumps\({[^}]+}\))\s+\n\s+# ==========================='

    if re.search(execute_tool_end_pattern, modified):
        modified = re.sub(
            execute_tool_end_pattern,
            new_tool_cases + r'\n\1\n\n# ===========================',
            modified
        )
        print("   ‚úÖ execute_tool extended")
    else:
        print("   ‚ö†Ô∏è  execute_tool end pattern not found - manual insertion needed")

    #=========================================================================
    # STEP 5: Update system prompt
    #=========================================================================
    print("\n5Ô∏è‚É£  Updating system prompt...")

    # Find and replace the system_prompt variable
    system_prompt_start = 'system_prompt = """'
    system_prompt_end = '"""'

    # Updated system prompt (condensed version)
    new_system_prompt = '''system_prompt = """
<identity>
You are the OneSuite Core Architect Agent (OCAA) - a unified multi-capability AI agent.

Roles: Product Manager, QA Automation Specialist, Production Monitor, Workflow Architect

When asked who you are: "I am the OneSuite Core Architect Agent (OCAA), a unified AI agent
specializing in product strategy, QA automation, error monitoring, and workflow execution."
</identity>

<capabilities>
1. Product Strategy - User stories, roadmaps, multi-channel analysis
2. QA Testing (Computer Use) - Automated UI testing, screenshot analysis, test reports
3. Error Monitoring - Log analysis, root cause analysis, automated fixes
4. Workflows - Evaluator-Optimizer, RAG pipelines, automation
5. Document Management (MCP) - PDF/DOCX conversion, storage, retrieval
</capabilities>

<tools_available>
Product: get_current_datetime, set_reminder, get_confluence_page, calculate_product_metrics
Documents (MCP): document_path_to_markdown, list_documents, read_document, update_document
Computer Use: take_screenshot, execute_computer_action (click, type, key_press, mouse_move)
Error Monitoring: analyze_production_error
QA: run_qa_workflow
</tools_available>

<instructions>
PRODUCT STRATEGY: Analyze scope ‚Üí Identify stakeholders ‚Üí Assess state ‚Üí Brainstorm ‚Üí Structure ‚Üí Validate
QA TESTING: Understand requirements ‚Üí Screenshot ‚Üí Execute tests ‚Üí Document ‚Üí Report bugs
ERROR MONITORING: Load logs ‚Üí Analyze root cause ‚Üí Generate fix ‚Üí Explain prevention
WORKFLOWS: Design ‚Üí Execute producer ‚Üí Run grader ‚Üí Provide feedback ‚Üí Iterate
</instructions>

<response_format>
Product: Context, Problem, Solution, Acceptance Criteria, Channel Impact, Dependencies
QA: Summary, Detailed Results, Bugs, Screenshots, Recommendations
Errors: Summary, Root Cause, Fix, Prevention, Validation
Workflows: Type, Steps, Results, Iterations, Lessons
</response_format>

<constraints>
- Focus on OneSuite Core for product tasks
- Use Computer Use responsibly
- Analyze errors thoroughly
- Document assumptions clearly
- Be specific and measurable
- Include multi-channel impacts
</constraints>

<communication_style>
Professional, structured, results-oriented. Precise for technical tasks, comprehensive for strategy.
</communication_style>
"""'''

    # Replace system_prompt
    pattern = r'system_prompt = """.*?"""'
    match = re.search(pattern, modified, re.DOTALL)
    if match:
        modified = modified.replace(match.group(), new_system_prompt)
        print("   ‚úÖ System prompt updated")
    else:
        print("   ‚ö†Ô∏è  System prompt not found - manual update needed")

    #=========================================================================
    # STEP 6: Add new commands to main()
    #=========================================================================
    print("\n6Ô∏è‚É£  Adding new commands to main()...")

    # Find the print statements in main() and add new ones
    commands_addition = '''
        print("  '/qa-test <url>' - Run automated QA tests")
        print("  '/test-mention' - Test @mention component")
        print("  '/monitor-errors' - Analyze production logs")
        print("  '/workflow-demo' - Evaluator-Optimizer demo")
        print("  '/full-automation' - Run all capabilities")
        print(f"\\n  Computer Use: {'‚úÖ' if COMPUTER_USE_AVAILABLE else '‚ùå'}")
'''

    # Find a suitable insertion point - after the existing command prints
    cmd_marker = '        print("  \'/format <doc_id>\' - Reformat a document in Markdown (MCP Prompt)")'
    if cmd_marker in modified:
        modified = modified.replace(cmd_marker, cmd_marker + "\n" + commands_addition)
        print("   ‚úÖ Commands added to menu")
    else:
        print("   ‚ö†Ô∏è  Command menu marker not found")

    #=========================================================================
    # STEP 7: Add command handlers (large block)
    #=========================================================================
    print("\n7Ô∏è‚É£  Adding command handlers...")

    # This is the largest section - command handling logic
    # I'll insert it before the final main loop continuation

    command_handlers = '''

            elif user_input.startswith("/qa-test"):
                parts = user_input.split(maxsplit=1)
                url = parts[1] if len(parts) > 1 else "http://localhost:8000"
                print(f"\\nüß™ QA Testing: {url}")
                result = tool_run_qa_workflow(url)
                if result.get("success"):
                    print(f"‚úÖ Complete - {result.get('report_path')}")
                else:
                    print(f"‚ùå Failed: {result.get('error')}")

            elif user_input == "/test-mention":
                print("\\nüß™ Testing @mention component...")
                test_cases = [
                    "Type '@' and verify autocomplete",
                    "Press Enter to insert mention",
                    "Test backspace positioning",
                    "Test Escape key",
                    "Test arrow navigation"
                ]
                result = tool_run_qa_workflow("http://localhost:8000", test_cases=test_cases)
                if result.get("success"):
                    print(f"‚úÖ Complete - {result['report_path']}")

            elif user_input == "/monitor-errors":
                print("\\nüêõ Monitoring errors...")
                errors = load_and_analyze_errors()
                print(f"Found {errors['errors_found']} errors")
                for i, err in enumerate(errors['errors'][:3], 1):
                    print(f"\\n{i}. {err['file']}:{err['line_num']}")
                    print(f"   {err['content']}")
                    analysis = tool_analyze_production_error(err['content'])
                    if analysis.get('success') and 'analysis' in analysis:
                        fix = analysis['analysis']
                        print(f"   Fix: {fix.get('fix', {}).get('description', 'N/A')[:60]}...")

            elif user_input == "/workflow-demo":
                print("\\nüîÑ Evaluator-Optimizer Demo")
                result = evaluator_optimizer_workflow(
                    "Write a product roadmap",
                    "Write a concise 3-month OneSuite roadmap (under 200 words)",
                    "Grade: PASS if has MVP/V1/Scale phases and under 200 words, else FAIL",
                    max_iterations=3
                )
                if result['success']:
                    print(f"\\n‚úÖ Succeeded in {result['iterations']} iterations")
                    print(f"\\nOutput:\\n{result['output'][:300]}...")
                else:
                    print(f"\\n‚ö†Ô∏è  {result['reason']}")

            elif user_input == "/full-automation":
                print("\\n" + "="*70)
                print("üéØ FULL AUTOMATION MODE")
                print("="*70)
                confirm = input("Run QA + Errors + Workflow? [y/N]: ")
                if confirm.lower() == 'y':
                    # QA
                    print("\\n1. QA Testing...")
                    try:
                        qa = tool_run_qa_workflow("http://localhost:8000")
                        print(f"   ‚úÖ {qa.get('report_path')}")
                    except Exception as e:
                        print(f"   ‚ùå {e}")

                    # Errors
                    print("\\n2. Error Monitoring...")
                    try:
                        errs = load_and_analyze_errors()
                        print(f"   ‚úÖ {errs['errors_found']} errors found")
                    except Exception as e:
                        print(f"   ‚ùå {e}")

                    # Workflow
                    print("\\n3. Workflow Demo...")
                    try:
                        wf = evaluator_optimizer_workflow(
                            "Test workflow",
                            "Write: As a user, I want...",
                            "PASS if has user story format",
                            max_iterations=2
                        )
                        print(f"   ‚úÖ {wf['success']}")
                    except Exception as e:
                        print(f"   ‚ùå {e}")

                    print("\\nüéâ Automation complete!")
'''

    # Find insertion point - after the /format command handler
    format_handler_pattern = r'(            # -+ rest of main loop -+)'
    # Actually, let's find a better marker - right before "else:" for user input

    # Find the last command handler before the main else
    insertion_point = "            else:\n                # Regular chat"
    if insertion_point in modified:
        modified = modified.replace(
            insertion_point,
            command_handlers + "\n" + insertion_point
        )
        print("   ‚úÖ Command handlers added")
    else:
        # Try alternate marker
        insertion_point2 = "        # Rest of chat loop"
        if insertion_point2 in modified:
            modified = modified.replace(
                insertion_point2,
                command_handlers + "\n            " + insertion_point2
            )
            print("   ‚úÖ Command handlers added (alternate location)")
        else:
            print("   ‚ö†Ô∏è  Command handler insertion point not found - manual needed")

    #=========================================================================
    # SAVE MERGED FILE
    #=========================================================================
    print("\n" + "="*70)
    print("üíæ Saving merged file...")

    output_path = Path(__file__).parent / "demo_unified.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(modified)

    print(f"‚úÖ Saved to: {output_path}")
    print(f"   New size: {len(modified)} characters")
    print(f"   Size increase: +{len(modified) - len(demo_content)} characters")

    #=========================================================================
    # SUMMARY
    #=========================================================================
    print("\n" + "="*70)
    print("üéâ MERGE COMPLETE!")
    print("="*70)
    print("""
‚úÖ Added to demo_unified.py:
   - Computer Use imports and initialization
   - Tool schemas (screenshot, computer action, error analysis, QA workflow)
   - Tool implementation functions
   - Extended execute_tool with new cases
   - Updated system prompt with all capabilities
   - New commands: /qa-test, /test-mention, /monitor-errors, /workflow-demo, /full-automation
   - Command handlers for all new features

üöÄ TO RUN:
   python demo_unified.py

üìã NEW COMMANDS:
   /qa-test <url>      - Run automated QA tests
   /test-mention       - Test @mention component
   /monitor-errors     - Analyze production logs
   /workflow-demo      - Evaluator-Optimizer pattern demo
   /full-automation    - Run all capabilities in sequence

‚ö†Ô∏è  REQUIREMENTS:
   pip install pillow pyautogui pdfplumber python-docx

üìù NOTES:
   - Original demo.py is unchanged
   - demo_unified.py is the new merged version
   - All existing OCAA features preserved
   - All unified agent features integrated
    """)

if __name__ == "__main__":
    main()
